# ğŸ¬ YouTube RAG Assistant

An app that lets you ask questions about any YouTube video using **Retrieval-Augmented Generation (RAG)**. Built entirely with **open-source technologies**, it runs locallyâ€”no cloud APIs, no subscriptions, and no hidden components.

This project was designed as a portfolio showcase to demonstrate practical usage of:
- **LangChain** for orchestration and text splitting
- **Hugging Face Sentence Transformers** for embeddings
- **FAISS** for vector storage and retrieval
- **Ollama** running **Mistral** as a local LLM
- **Streamlit** as a lightweight and interactive UI

---

## ğŸ§  Key Concepts

| Concept | Description |
|--------|-------------|
| RAG (Retrieval-Augmented Generation) | Combines external knowledge (YouTube transcript) with LLM generation |
| Text Splitting | Breaks the transcript into overlapping chunks using LangChain to improve embedding and retrieval accuracy. |
| Embedding | Converts text into vectors to enable semantic search |
| Vector Store | Stores and retrieves chunks relevant to your query |
| Local LLM | Uses `mistral` model via `ollama`, no internet or API key needed |

---

## ğŸ›  Tech Stack

- **LangChain** â€“ chaining logic, text splitting
- **Hugging Face (all-MiniLM-L6-v2)** â€“ fast, free embeddings
- **FAISS** â€“ vector similarity search
- **Ollama** â€“ runs `mistral` locally
- **YouTube Transcript API** â€“ fetches transcript from public videos
- **Streamlit** â€“ interactive browser UI
- **Python 3.10+**

> ğŸš« 100% Open Source â€” no OpenAI keys, no paywalls, no trackers

---

## ğŸ“ Project Structure
```
youtube-rag-assistant/
â”œâ”€â”€ main.py # CLI entry point (for testing)
â”œâ”€â”€ ui/app.py # Streamlit UI
â”œâ”€â”€ config/settings.py # Global settings (chunk size, model names)
â”œâ”€â”€ data/ # Stores transcripts and FAISS vectorstores
â”‚ â”œâ”€â”€ transcripts/
â”‚ â””â”€â”€ vectorstore/
â”œâ”€â”€ src/ # Core logic modules
â”‚ â”œâ”€â”€ youtube_loader.py # Transcript fetcher
â”‚ â”œâ”€â”€ text_splitter.py # Chunking logic
â”‚ â”œâ”€â”€ embedder.py # Embedding + FAISS builder
â”‚ â”œâ”€â”€ retriever.py # Loads retriever
â”‚ â””â”€â”€ qa_chain.py # LLM + RAG chain setup
```
---

## â–¶ï¸ How to Run
```bash
ğŸ§± 1. Clone the repository
git clone https://github.com/your-username/youtube-rag-assistant.git
cd youtube-rag-assistant

ğŸ 2. Create and activate a virtual environment
conda create -n rag python=3.10 -y
conda activate rag

ğŸ“¦ 3. Install dependencies
pip install -r requirements.txt
Make sure ollama is installed and the mistral model is pulled:
ollama run mistral

ğŸŒ 4. Launch the app
streamlit run ui/app.py
Then open your browser at: http://localhost:8501
```

## ğŸ§ª Example Workflow
Paste a YouTube video link (wich allows transcript)

The transcript is fetched and split into chunks

Chunks are embedded using Hugging Face model

Stored and indexed with FAISS vectorstore

Ask questions â†’ the app retrieves relevant chunks and sends them to the Mistral LLM running locally via Ollama

## ğŸ“ Educational Note
This codebase contains multiple print() statements intentionally left for debugging and educational purposes.
These help you see whatâ€™s happening step-by-step behind the scenes (e.g. chunk counts, vectorstore paths, retrieved chunks).

Feel free to clean or extend as needed for your own applications.